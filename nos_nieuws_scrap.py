import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from collections import Counter
import csv
import time
import re
import random
import io

st.set_page_config(page_title="NOS ë‰´ìŠ¤ í¬ë¡¤ëŸ¬", layout="wide")
st.title("ğŸ“° NOS ë‰´ìŠ¤ í¬ë¡¤ë§ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")

# --------------------- ë³¸ë¬¸ í¬ë¡¤ë§ ---------------------
def get_article_body(url):
    stop_words = {
        "Deel artikel", "Voorpagina", "Laatste nieuws", "Video's", "Binnenland",
        "Buitenland", "Regionaal nieuws", "Politiek", "Economie", "Koningshuis",
        "Tech", "Cultuur & media",  "Cultuur & Media","Opmerkelijk","In samenwerking met RTV Utrecht", "In samenwerking met NH"
    }

    try:
        headers = {"User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
        resp = requests.get(url, headers=headers)
        time.sleep(random.uniform(1.5, 2.5))
        soup = BeautifulSoup(resp.text, "html.parser")

        main = soup.select_one("main#content")
        if not main:
            return "ë³¸ë¬¸ ì—†ìŒ"

        parts = []
        for el in main.find_all(["p", "h2", "li"], recursive=True):
            txt = el.get_text(" ", strip=True)
            if not txt:
                continue
            if txt in stop_words:
                break
            if el.name == "h2":
                parts.append(f"\n**{txt}**\n")
            elif el.name == "li":
                parts.append(f"- {txt}")
            else:
                parts.append(txt)

        return "\n\n".join(parts)

    except Exception as e:
        return str(e)

# --------------------- í•˜ë“œì½”ë”© ë„¤ëœë€ë“œì–´ ë¶ˆìš©ì–´ ---------------------
dutch_stopwords = {
    "de", "en", "van", "ik", "te", "dat", "die", "in", "een", "hij", "het", "niet",
    "zijn", "is", "was", "op", "aan", "met", "als", "voor", "had", "er", "maar",
    "om", "hem", "dan", "zou", "of", "wat", "mijn", "men", "dit", "zo", "door",
    "over", "ze", "zich", "bij", "ook", "tot", "je", "mij", "uit", "der", "daar",
    "haar", "naar", "heb", "hoe", "heeft", "hebben", "deze", "u", "want", "nog",
    "zal", "me", "zij", "nu", "ge", "geen", "omdat", "iets", "worden", "toch",
    "al", "waren", "veel", "meer", "doen", "toen", "moet", "ben", "zonder", "kan",
    "hun", "dus", "alles", "onder", "ja", "werd", "wezen", "zelf", "tegen",
    "hebben", "waar", "zal", "komen", "tegen", "goed", "hier", "wie", "waarom"
}

# --------------------- í‚¤ì›Œë“œ ì¶”ì¶œ ---------------------
def extract_keywords(text, top_n=10):
    words = re.findall(r'\b[a-zA-Z]{10,}\b', text)
    capitalized_words = [w.capitalize() for w in words]
    filtered_words = [w for w in capitalized_words if w.lower() not in dutch_stopwords]
    freq = Counter(filtered_words)

    unique_keywords = []
    for word, _ in freq.most_common():
        if word not in unique_keywords:
            unique_keywords.append(word)
        if len(unique_keywords) >= top_n:
            break

    return [(kw, freq[kw]) for kw in unique_keywords], filtered_words

# --------------------- ë‰´ìŠ¤ í¬ë¡¤ë§ (requests + bs4) ---------------------
def crawling_news(category_slug, count=2):
    base_url = "https://nos.nl/nieuws/"
    category_url = base_url + category_slug

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120 Safari/537.36"
    }

    news_list = []

    try:
        resp = requests.get(category_url, headers=headers)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ê¸°ì‚¬ ë§í¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ - ëŒ€ëµì ì¸ CSS ì„ íƒì ì˜ˆì‹œ, í•„ìš”ì— ë”°ë¼ ì¡°ì •
        articles = soup.select("a[href*='/artikel']")  

        urls_seen = set()

        for link in articles:
            if len(news_list) >= count:
                break

            url = link.get('href')
            if not url or url in urls_seen:
                continue
            if not url.startswith("http"):
                url = "https://nos.nl" + url
            urls_seen.add(url)

            title = link.get_text(strip=True)
            titleSet = title.split('\n')
            try:
                body = get_article_body(url)
                keywords, long_words = extract_keywords(body)
                news_list.append({
                    'title': titleSet[1] if len(titleSet) > 1 else titleSet[0],
                    'url': url,
                    'body': body,
                    'keywords': [kw for kw, _ in keywords],
                    'lange_woorden': long_words
                })
                time.sleep(random.uniform(2, 4))
            except:
                continue
    except Exception as e:
        st.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    return news_list

# --------------------- CSV ìƒì„± ---------------------
def generate_csv_bytes(result):
    if not result:
        return None
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=['index', 'title', 'url', 'keywords', 'lange_woorden'])
    writer.writeheader()
    for idx, news in enumerate(result, 1):
        writer.writerow({
            'index': idx,
            'title': news['title'],
            'url': news['url'],
            'keywords': ', '.join(news['keywords']),
            'lange_woorden': ', '.join(news['lange_woorden'])
        })
    return output.getvalue().encode('utf-8-sig')

# --------------------- ì¹´í…Œê³ ë¦¬ ëª©ë¡ ---------------------
menu_dict = {
    1:"voorpagina", 2:"laatste-nieuws", 3:"videos", 4:"binnenland", 5:"buitenland",
    6:"regionaal-nieuws", 7:"politiek", 8:"economie", 9:"koningshuis",
    10:"tech", 11:"cultuur-en-media", 12:"opmerkelijk"
}

# --------------------- UI ---------------------
selected = st.selectbox("ğŸ—‚ï¸ Kies een ì¹´í…Œê³ ë¦¬", list(menu_dict.keys()), format_func=lambda x: f"{x}. {menu_dict[x]}")
article_count = st.slider("ğŸ“° ê¸°ì‚¬ ìˆ˜ ì„ íƒ", 1, 10, 2)

# --------------------- ì‹¤í–‰ ë²„íŠ¼ ---------------------
if st.button("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘"):
    result = crawling_news(menu_dict[selected], article_count)

    for i, news in enumerate(result, 1):
        st.markdown(f"### {i}. {news['title']}")
        st.markdown(f"ğŸ”— [ê¸°ì‚¬ ë§í¬]({news['url']})")
        st.markdown(f"ğŸ§  **í‚¤ì›Œë“œ:** {', '.join(news['keywords'])}")
        with st.expander("ğŸ“„ ë³¸ë¬¸ í¼ì¹˜ê¸°"):
            st.write(news['body'])

    if st.checkbox("ğŸ“„ CSV íŒŒì¼ì„ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
        csv_bytes = generate_csv_bytes(result)
        if csv_bytes:
            st.success("âœ… CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.download_button(
                label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_bytes,
                file_name=f"nos_nieuws_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv"
            )
        else:
            st.warning("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
