import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from collections import Counter
import csv
import time
import re
import random
import io

st.set_page_config(page_title="NOS ë‰´ìŠ¤ í¬ë¡¤ëŸ¬", layout="wide")
st.title("ğŸ“° NOS ë‰´ìŠ¤ í¬ë¡¤ë§ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")

# --------------------- ê¸°ì‚¬ ì œëª© + ë³¸ë¬¸ í•œ ë²ˆì— ì¶”ì¶œ ---------------------
def get_article_info(url):
    stop_words = {
        "Deel artikel", "Voorpagina", "Laatste nieuws", "Video's", "Binnenland",
        "Buitenland", "Regionaal nieuws", "Politiek", "Economie", "Koningshuis",
        "Tech", "Cultuur & media", "Cultuur & Media", "Opmerkelijk",
        "In samenwerking met RTV Utrecht", "In samenwerking met NH"
    }

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
        }
        resp = requests.get(url, headers=headers)
        time.sleep(random.uniform(1.0, 2.0))  # ë³¸ë¬¸ ìš”ì²­ í›„ ì ê¹ ëŒ€ê¸°
        soup = BeautifulSoup(resp.text, "html.parser")

        main = soup.select_one("main#content")
        if not main:
            return "ì œëª© ì—†ìŒ", "ë³¸ë¬¸ ì—†ìŒ"

        # ì œëª© ì¶”ì¶œ
        h1 = main.find("h1")
        title = h1.get_text(" ", strip=True) if h1 else "ì œëª© ì—†ìŒ"

        # ë³¸ë¬¸ ì¶”ì¶œ
        parts = []
        for el in main.find_all(["p", "h2", "li"], recursive=True):
            txt = el.get_text(" ", strip=True)
            if not txt:
                continue
            if txt in stop_words:
                break
            if title and txt == title:
                continue
            if el.name == "h2":
                parts.append(f"\n**{txt}**\n")
            elif el.name == "li":
                parts.append(f"- {txt}")
            else:
                parts.append(txt)

        body = "\n\n".join(parts)
        return title, body

    except Exception as e:
        return "ì œëª© ì—†ìŒ", str(e)

# --------------------- ë¶ˆìš©ì–´ ---------------------
dutch_stopwords = {
    "de", "en", "van", "ik", "te", "dat", "die", "in", "een", "hij", "het", "niet",
    "zijn", "is", "was", "op", "aan", "met", "als", "voor", "had", "er", "maar",
    "om", "hem", "dan", "zou", "of", "wat", "mijn", "men", "dit", "zo", "door",
    "over", "ze", "zich", "bij", "ook", "tot", "je", "mij", "uit", "der", "daar",
    "haar", "naar", "heb", "hoe", "heeft", "hebben", "deze", "u", "want", "nog",
    "zal", "me", "zij", "nu", "ge", "geen", "omdat", "iets", "worden", "toch",
    "al", "waren", "veel", "meer", "doen", "toen", "moet", "ben", "zonder", "kan",
    "hun", "dus", "alles", "onder", "ja", "werd", "wezen", "zelf", "tegen",
    "komen", "goed", "hier", "wie", "waarom"
}

# --------------------- í‚¤ì›Œë“œ ì¶”ì¶œ ---------------------
def extract_keywords(text, top_n=10):
    words = re.findall(r'\b[a-zA-Z]{10,}\b', text)
    capitalized_words = [w.capitalize() for w in words]
    filtered_words = [w for w in capitalized_words if w.lower() not in dutch_stopwords]
    freq = Counter(filtered_words)

    unique_keywords = []
    for word, _ in freq.most_common():
        if word not in unique_keywords:
            unique_keywords.append(word)
        if len(unique_keywords) >= top_n:
            break

    return [(kw, freq[kw]) for kw in unique_keywords], filtered_words

# --------------------- ë‰´ìŠ¤ í¬ë¡¤ë§ ---------------------
def crawling_news(category_slug, count=2):
    base_url = "https://nos.nl/"
    category_url = base_url + category_slug
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120 Safari/537.36"
    }

    news_list = []
    try:
        resp = requests.get(category_url, headers=headers)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select("a[href*='/artikel']")
        urls_seen = set()

        # ì§„í–‰ ìƒí™© ë°”
        progress = st.progress(0)
        total = min(count, len(articles))

        for idx, link in enumerate(articles):
            if len(news_list) >= count:
                break

            url = link.get('href')
            if not url or url in urls_seen:
                continue
            if not url.startswith("http"):
                url = "https://nos.nl" + url
            urls_seen.add(url)

            try:
                title, body = get_article_info(url)
                keywords, long_words = extract_keywords(body)

                news_list.append({
                    'title': title,
                    'url': url,
                    'body': body,
                    'keywords': [kw for kw, _ in keywords],
                    'lange_woorden': long_words
                })

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress.progress(int((len(news_list) / total) * 100))

                # ê¸°ì‚¬ í•˜ë‚˜ ì²˜ë¦¬ í›„ ëŒ€ê¸°
                time.sleep(random.uniform(2.0, 3.0))

            except:
                continue

        progress.empty()  # ì§„í–‰ ìƒí™© ë°” ì œê±°

    except Exception as e:
        st.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        st.write(f"âŒ ì‹¤íŒ¨í•œ URL: {category_url}")

    return news_list

# --------------------- CSV ìƒì„± ---------------------
def generate_csv_bytes(result):
    if not result:
        return None
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=['index', 'title', 'url', 'keywords', 'lange_woorden'])
    writer.writeheader()
    for idx, news in enumerate(result, 1):
        writer.writerow({
            'index': idx,
            'title': news['title'],
            'url': news['url'],
            'keywords': ', '.join(news['keywords']),
            'lange_woorden': ', '.join(news['lange_woorden'])
        })
    return output.getvalue().encode('utf-8-sig')

# --------------------- UI ì¹´í…Œê³ ë¦¬ ---------------------
menu_dict = { 
    1: "Laatste nieuws",
    2: "Video's",
    3: "Binnenland",
    4: "Buitenland",
    5: "Regionaal nieuws",
    6: "Politiek",
    7: "Economie",
    8: "Koningshuis",
    9: "Tech",
    10: "Cultuur & media",
    11: "Opmerkelijk"
}

menu_url_map = {
    "Laatste nieuws": "nieuws/laatste",
    "Video's": "nieuws/laatste/videos",
    "Binnenland": "nieuws/binnenland",
    "Buitenland": "nieuws/buitenland",
    "Regionaal nieuws": "nieuws/regio",
    "Politiek": "nieuws/politiek",
    "Economie": "nieuws/economie",
    "Koningshuis": "nieuws/koningshuis",
    "Tech": "nieuws/tech",
    "Cultuur & media": "nieuws/cultuur-en-media",
    "Opmerkelijk": "nieuws/opmerkelijk"
}

# --------------------- UI ---------------------
selected = st.selectbox("ğŸ—‚ï¸ Kies een ì¹´í…Œê³ ë¦¬", list(menu_dict.keys()), format_func=lambda x: f"{x}. {menu_dict[x]}")
article_count = st.slider("ğŸ“° ê¸°ì‚¬ ìˆ˜ ì„ íƒ", 1, 10, 2)

if st.button("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘"):
    st.info("ğŸ”„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
    selected_name = menu_dict[selected]
    category_slug = menu_url_map.get(selected_name, "")
    result = crawling_news(category_slug, article_count)

    for i, news in enumerate(result, 1):
        st.markdown(f"### {i}. {news['title']}")
        st.markdown(f"ğŸ”— [ê¸°ì‚¬ ë§í¬]({news['url']})")
        st.markdown(f"ğŸ§  **í‚¤ì›Œë“œ:** {', '.join(news['keywords'])}")
        with st.expander("ğŸ“„ ë³¸ë¬¸ í¼ì¹˜ê¸°"):
            st.write(news['body'])

    if result and st.checkbox("ğŸ“„ CSV íŒŒì¼ì„ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
        csv_bytes = generate_csv_bytes(result)
        if csv_bytes:
            st.success("âœ… CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.download_button(
                label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_bytes,
                file_name=f"nos_nieuws_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv"
            )
        else:
            st.warning("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
