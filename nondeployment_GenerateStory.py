import os
from dotenv import load_dotenv
import streamlit as st
from PyPDF2 import PdfReader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import google.generativeai as genai
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. í™˜ê²½ë³€ìˆ˜ & Gemini API ì„¤ì •
load_dotenv()
API_KEY = os.environ.get("GEMINI_API_KEY")
if not API_KEY:
    st.error("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš” (.env)")
    st.stop()

genai.configure(api_key=API_KEY)

# 2. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ë””ë²„ê¹… ì½”ë“œ í¬í•¨)
def extract_text_from_pdf(file) -> str:
    pdf = PdfReader(file)
    texts = []
    for i, page in enumerate(pdf.pages):
        text = page.extract_text()
        if text:
            texts.append(text)
        else:
            st.warning(f"âš ï¸ {i+1}í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    full_text = "\n".join(texts)

    # âœ… ë””ë²„ê¹…: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ ì—¬ë¶€ ì¶œë ¥
    if full_text.strip():
        st.success(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ì´ {len(full_text)}ì)")
        st.expander("ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°").write(full_text[:1000])  # ì•ë¶€ë¶„ë§Œ í‘œì‹œ
    else:
        st.error("âŒ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDFê°€ ì´ë¯¸ì§€ ê¸°ë°˜ì´ê±°ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return full_text

# 3. ì„ë² ë”© & FAISS ìƒì„± í•¨ìˆ˜
def create_faiss_index(text, embedding_model):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)

    # âœ… ë””ë²„ê¹…: ë¶„í• ëœ ì²­í¬ ê°œìˆ˜ í™•ì¸
    st.info(f"ğŸ” ë¶„í• ëœ í…ìŠ¤íŠ¸ ì¡°ê° ìˆ˜: {len(chunks)}")

    db = FAISS.from_texts(chunks, embedding_model)
    return db

# 4. Gemini í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_text_with_gemini(prompt: str, token_limit=300) -> str:
    model = genai.GenerativeModel("gemini-1.5-flash")
    generation_config = genai.GenerationConfig(max_output_tokens=token_limit)
    response = model.generate_content(
        prompt,
        generation_config=generation_config
    )
    return response.text

# 5. ìŠ¤í† ë¦¬ë¼ì¸ ìƒì„±
def generate_storylines(db):
    context_docs = db.similarity_search("ìŠ¤í† ë¦¬ë¼ì¸ ì¶”ì²œ", k=5)
    combined_context = "\n".join([doc.page_content for doc in context_docs])

    prompt = (
        f"ë‹¤ìŒì€ PDFì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤:\n{combined_context}\n\n"
        f"ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í† ë¦¬ë¼ì¸ 5ê°€ì§€ë¥¼ ì¶”ì²œí•´ì¤˜. "
        f"ê° ìŠ¤í† ë¦¬ë¼ì¸ì€ ìµœëŒ€ 3ë¬¸ì¥, ë¬¸ì¥ë‹¹ 50ìë¥¼ ë„˜ì§€ ì•Šì•„ì•¼ í•´."
    )
    
    # ë””ë²„ê·¸ìš©: í”„ë¡¬í”„íŠ¸ í™”ë©´ ì¶œë ¥
    st.text_area("â–¶ Gemini APIë¡œ ë³´ë‚´ëŠ” í”„ë¡¬í”„íŠ¸ (ë””ë²„ê·¸)", prompt, height=300)
    
    return generate_text_with_gemini(prompt, token_limit=1000)

# 6. ì „ì²´ ìŠ¤í† ë¦¬ ìƒì„±
def generate_full_story(db, selected_storyline, token_limit):
    context_docs = db.similarity_search(selected_storyline, k=10)
    if not context_docs:
        st.warning("ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í† ë¦¬ë¼ì¸ì„ ë°”ê¿”ë³´ì„¸ìš”.")
        return "âš ï¸ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ"

    combined_context = "\n".join([doc.page_content for doc in context_docs])

    # âœ… ë””ë²„ê¹…: ê²€ìƒ‰ëœ context ì¼ë¶€ í™•ì¸
    st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ë§¥ ë¯¸ë¦¬ë³´ê¸°").write(combined_context[:1000])

    prompt = (
        f"ë‹¤ìŒì€ PDFì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤:\n{combined_context}\n\n"
        f"ì„ íƒëœ ìŠ¤í† ë¦¬ë¼ì¸: {selected_storyline}\n\n"
        f"ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì„ íƒëœ ìŠ¤í† ë¦¬ë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ {token_limit} í† í° ë¶„ëŸ‰ì˜ ì „ì²´ ìŠ¤í† ë¦¬ë¥¼ ì‘ì„±í•´ì¤˜. "
        f"ìŠ¤í† ë¦¬ëŠ” ë°œë‹¨, ì „ê°œ, ìœ„ê¸°, ì ˆì •, ê²°ë§ì˜ 5ë‹¨ê³„ë¡œ êµ¬ì„±í•˜ê³ , ê° ë‹¨ê³„ë¥¼ ë™ë“±í•œ ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•´ì¤˜. "
        f"ìµœëŒ€í•œ {token_limit} í† í° ë¶„ëŸ‰ë§Œí¼ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ì‚¬ìš©í•´ì¤˜ "
        f"ì „ì²´ ìŠ¤í† ë¦¬ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´."
    )
    return generate_text_with_gemini(prompt, token_limit=token_limit)

# --- Streamlit UI ì‹œì‘ ---
st.title("ğŸ“š AIë¡œ ì†Œì„¤ ì“°ê¸° (Gemini API ì‚¬ìš©)")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "db" not in st.session_state:
    st.session_state.db = None
if "pdf_text" not in st.session_state:
    st.session_state.pdf_text = ""
if "messages" not in st.session_state:
    st.session_state.messages = []
if "storylines" not in st.session_state:
    st.session_state.storylines = ""

uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file is not None and st.session_state.db is None:
    with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
        text = extract_text_from_pdf(uploaded_file)
        st.session_state.pdf_text = text

    if text.strip():
        with st.spinner("ì„ë² ë”© ë° ê²€ìƒ‰ DB êµ¬ì¶• ì¤‘..."):
            embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
            db = create_faiss_index(text, embedding_model)
            st.session_state.db = db
            st.success("âœ… PDF ë¶„ì„ ë° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

if st.session_state.db:
    st.info("PDFê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œ ë° ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if st.button("ìŠ¤í† ë¦¬ë¼ì¸ ì¶”ì²œ ìƒì„±"):
        with st.spinner("ìŠ¤í† ë¦¬ë¼ì¸ ì¶”ì²œ ìƒì„± ì¤‘..."):
            storylines = generate_storylines(st.session_state.db)
            st.session_state.storylines = storylines

    if st.session_state.storylines:
        st.markdown("### ì¶”ì²œ ìŠ¤í† ë¦¬ë¼ì¸ 5ê°œ")
        st.markdown(st.session_state.storylines)

        st.markdown("---")
        st.markdown("### âœï¸ ì „ì²´ ì†Œì„¤ ì‘ì„±")

        storyline_list = [line for line in st.session_state.storylines.strip().split('\n') if line.strip()]
        selected_storyline = st.selectbox("ì „ì²´ ìŠ¤í† ë¦¬ë¥¼ ì‘ì„±í•  ìŠ¤í† ë¦¬ë¼ì¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", storyline_list)

        story_token_limit = st.slider("ì „ì²´ ìŠ¤í† ë¦¬ì˜ ìµœëŒ€ í† í° ìˆ˜ (1000 ~ 10000)", min_value=1000, max_value=10000, value=2000, step=100)

        if st.button("ì „ì²´ ì†Œì„¤ ì‘ì„± ì‹œì‘"):
            with st.spinner("ì „ì²´ ì†Œì„¤ ìƒì„± ì¤‘..."):
                full_story = generate_full_story(st.session_state.db, selected_storyline, story_token_limit)
                st.session_state.messages.append({"role": "assistant", "content": full_story})
                st.rerun()

    if st.button("ì´ì–´í•˜ê¸°"):
        if st.session_state.messages and st.session_state.messages[-1]["role"] == "assistant":
            last_text = st.session_state.messages[-1]["content"]
            continue_prompt = last_text + "\n\nì•ì„  ê²°ê³¼ì˜ ë’· ë¶€ë¶„ì„ ì¶”ê°€ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
            with st.spinner("ì´ì–´í•˜ê¸° ìƒì„± ì¤‘..."):
                continuation = generate_text_with_gemini(continue_prompt, token_limit=story_token_limit)
                st.session_state.messages[-1]["content"] += continuation
        else:
            st.warning("ì´ì–´í•  AI ì¶œë ¥ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg["content"]
        if role == "assistant":
            st.markdown(f"**AI:** {content}")
        else:
            st.markdown(f"**ì‚¬ìš©ì:** {content}")

    user_input = st.text_input("AIì—ê²Œ ìˆ˜ì • ìš”ì²­ì´ë‚˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”")
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        recent_msgs = st.session_state.messages[-6:]
        chat_context = ""
        for m in recent_msgs:
            prefix = "ì‚¬ìš©ì" if m["role"] == "user" else "AI"
            chat_context += f"{prefix}: {m['content']}\n"
        prompt = (
            f"ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ AIì˜ ì´ì „ ëŒ€í™”ì…ë‹ˆë‹¤:\n{chat_context}\n"
            f"ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ìš”ì²­ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        )
        reply = generate_text_with_gemini(prompt, token_limit=500)
        st.session_state.messages.append({"role": "assistant", "content": reply})
        st.rerun()
else:
    st.info(
        """PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ì†Œì„¤ ê° êµ¬ê°„ë³„ë¡œ ì°½ì‘í•´ë“œë¦½ë‹ˆë‹¤.\n
        - ì´ë¯¸ì§€ ê¸°ë°˜ PDFëŠ” ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n
        - Wordë‚˜ í•œê¸€ íŒŒì¼ì„ PDFë¡œ ì €ì¥ í›„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.\n
        - ë°°ê²½, ë“±ì¥ì¸ë¬¼, ì „ê°œ ë“±ì´ í¬í•¨ëœ ë‚´ìš©ì¼ìˆ˜ë¡ ì¢‹ì€ ê²°ê³¼ê°€ ìƒì„±ë©ë‹ˆë‹¤.
        """
    )

if st.button("ëŒ€í™” ë° ë°ì´í„° ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.db = None
    st.session_state.pdf_text = ""
    st.session_state.storylines = ""
    st.rerun()
